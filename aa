#%% Selecting Futures - Lags:

##################### 1 - method - backward elimination:
selected_features_BE = BackwardEliminationPvalue(\
                                    X_Train = X_Train, y_Train = y_Train)
  
    
##################### 2 - method - stepwise-bidirectional elimination:
selected_features_BE = stepwise_selection(X_Train, y_Train)    
      

#################### 3 - method - stepwise-bidirectional backward elimination:
selected_features_BE = BidirectionalStepwiseSelection(X_Train, y_Train, elimination_criteria = "adjr2")[0]   



#################### 4 - method - Genetive Algoritms:
# kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
# https://www.kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
from EvolutionaryFS import GeneticAlgorithmFS
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

data_dict={0:{'x_train':X_Train,'y_train':y_Train,'x_test':X_Test,'y_test':y_Test}}
columns_list=list(X_Train.columns)

model_object=LinearRegression(n_jobs=-1)
# model_object=LinearRegression(n_jobs=-1)
evoObj=GeneticAlgorithmFS(model=model_object,data_dict=data_dict,\
                          cost_function=mean_squared_error,\
                          average='',\
                          cost_function_improvement='decrease',\
                          columns_list=columns_list,\
                          generations=100,\
                          population=50,\
                          prob_crossover=0.9,\
                          prob_mutation=0.1,\
                          run_time=60000)
selected_features_BE__finall=evoObj.GetBestFeatures()    

#################### 4 - method - Recursive Feature Elimination
# https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
model = LinearRegression()


#no of features
nof_list=np.arange(1, X_Train.shape[1] )            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LinearRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_Train, y_Train)
    X_test_rfe = rfe.transform(X_Test)
    model.fit(X_train_rfe, y_Train)
    score = model.score(X_test_rfe, y_Test)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
        
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))


cols = list(X_Train.columns)
model = LinearRegression()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_Train,y_Train)  
#Fitting the data to model
model.fit(X_rfe,y_Train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

#%% Update X sets  and LagList

selected_features_BE__finall = \
KeepBasicIndeptVarAndDummies(X_Train, selected_features_BE,\
                                 DummyForCol = 'month',\
                                 KeepBasicIndept = True,\
                                 KeepDummies = True)


#%%

X_Train = X_Train.filter(items=selected_features_BE__finall)
X_Test = X_Test.filter(items=selected_features_BE__finall)

LagsList = { var : LagNr for var, LagNr in LagsList.items()\
                                    if var in selected_features_BE__finall}
                                    
                                    
                                    
 ######################################################################################
 
 def SplitIndexIncludingBatch(DF, BatchSize,\
                            TestSplitInd,\
                            ValSplitInd,\
                            WindowLength = None):
    '''
    Function return dates of Validation and Test Sets for selected Batch size.
    Accualy it return values closest to selected values.
    
    Function calculate length of the DataFrame including future cut by WindowLength.
    If data length is not divided by batch size function cut the data.
    Then function prepare length of Train set based on specified date. However, if length of 
    train set is not divided by batch size, than it cuts data from the end of set.
    
    Args:
        DF (pd.DataFrame): Oryginal DataFrame with all observations
        BatchSize (int): batch size
        TestSplitInd (str): String representing date of Start of Test Set
        ValSplitInd (str): String representing date of Start of Test Set
        WindowLength (int): length of lags widnow

    Returns:
        (Train.index, Val.index, Test.index) (tuple(indexdatetime, indexdatetime, indexdatetime)):
                3 indexs representing train, validaton and test sets
            
    '''
     
    
    
    DFfinall = DF.copy()
    TotalLength = DF.shape[0]

    if WindowLength is not None:
        DFfinall = DFfinall.iloc[(WindowLength-1):]
        TotalLength = DFfinall.shape[0]
        
    if TotalLength % BatchSize != 0:
#       print('Batch Size should be divisor of the data length')
        DFfinall = DFfinall.iloc[(TotalLength%BatchSize):]
        
    Train, RestDF  = TrainTestSets(DFfinall, ValSplitInd)
    if Train.shape[0] % BatchSize != 0:      
        Train = Train.iloc[:-(Train.shape[0] % BatchSize)]
        RestDF = DFfinall[Train.index[-1]:].iloc[1:]
    
    Val, Test = TrainTestSets(RestDF, TestSplitInd)
    if Val.shape[0] % BatchSize != 0:    
        Val = Val.iloc[:-(Val.shape[0] % BatchSize)]
        Test = RestDF[Val.index[-1]:].iloc[1:]    

    return (Train.index, Val.index, Test.index)


################################################
################################################
################################################

Train,Val, Test = CheckPossibilityOfBatch(AnalysisData, BatchSize = 133,\
                         TestSplitInd = TestSetDate,\
                         ValSplitInd = ValSetDate,\
                         WindowLength = 52)
    
    AnalysisData.head(50)
len(Train), len(Val), len(Test)








list(A1.index)[2:]
A1[A1.index[-3]:].iloc[1:]
    
def PrepareDataForLSTMregression(DataDF, DependentVar, IndependentVar,\
                                 TestSplitInd, \
                                 ValSplitInd,\
                                 BatchSize = None,\
                                 WindowLength = None,\
                                 WidndowForDependentVar = False,\
                                 DummyForCol = None,\
                                 ScalerType = None,\
                                 ScalerRange = (0,1)):

    # DependentVar <- str, IndependentVar <- list of str
    # ScalerType <- 'MinMax' or 'Standard'
    # ScalerRange = (0,1), or (-1,1)    - tuple
    
    DataToReturn = []
       
    # Select columns of DataFrame:    
    DF = DataDF.copy().loc[:, [DependentVar] + IndependentVar]
    
    # CREATE DUMMY VARIABLES FOR 'DummyForCol' COLUMNS

    DF = CreateDummyForColumns(DF, DummyForCol)
    
                
    ### SPLIT INTO DEPENDENT AND INDEPANEDNT VARIABLES
    DF_y = DF.loc[:, [DependentVar] ]
    DF_X = DF.drop(columns= [DependentVar] )
     
    
      
    ### DIVIDE INTO TRAIN, VALIDATION AND TEST SETS    
    TrainDF_y, TestDF_y = TrainTestSets(DF_y, TestSplitInd)
    TrainDF_X, TestDF_X = TrainTestSets(DF_X, TestSplitInd)
    if ValSplitInd is not None:
        TrainDF_y, ValDF_y  = TrainTestSets(TrainDF_y, ValSplitInd)
        TrainDF_X, ValDF_X  = TrainTestSets(TrainDF_X, ValSplitInd)


    # Return data if NO scaling or SCALE DATA:
    if ScalerType is None:
        # Create list of DF to return
        DataToReturn = [TrainDF_X, TrainDF_y,\
                        TestDF_X, TestDF_y]
        if ValSplitInd is not None:
            DataToReturn =\
                DataToReturn[:2] + [ValDF_X, ValDF_y] + DataToReturn[2:]
                
    elif ScalerType is not None:
        # Define Scalers:        
        if ScalerType == 'MinMax':    
            scaler_y = MinMaxScaler( feature_range = ScalerRange )
            scaler_X = MinMaxScaler( feature_range = ScalerRange )        
        elif ScalerType == 'Standard':    
            scaler_y = StandardScaler()
            scaler_X = StandardScaler()
    
        # Fit scalers on Trains Sets:
        scaler_y = scaler_y.fit(TrainDF_y)
        scaler_X = scaler_X.fit(TrainDF_X)    

        # Scale And Convert to DataFrame Train and Test sets:        
        Train_y_sld = ScaleThenConvertArrayToDF(TrainDF_y, scaler_y)
        Train_X_sld = ScaleThenConvertArrayToDF(TrainDF_X, scaler_X)

        Test_y_sld = ScaleThenConvertArrayToDF(TestDF_y, scaler_y)
        Test_X_sld = ScaleThenConvertArrayToDF(TestDF_X, scaler_X)
        
        # Create data to return:
        DataToReturn = [Train_X_sld, Train_y_sld,\
                        Test_X_sld, Test_y_sld,\
                        scaler_X, scaler_y]
        
        if ValSplitInd is not None:
            # Scale And Convert to DataFrame Val sets        
            Val_y_sld = ScaleThenConvertArrayToDF(ValDF_y, scaler_y)
            Val_X_sld = ScaleThenConvertArrayToDF(ValDF_X, scaler_X)
            
            # Extend data to return:
            DataToReturn =\
                DataToReturn[:2] + [Val_X_sld, Val_y_sld] + DataToReturn[2:]
          
                
    return tuple(DataToReturn)


