#%% Selecting Futures - Lags:

##################### 1 - method - backward elimination:
selected_features_BE = BackwardEliminationPvalue(\
                                    X_Train = X_Train, y_Train = y_Train)
  
    
##################### 2 - method - stepwise-bidirectional elimination:
selected_features_BE = stepwise_selection(X_Train, y_Train)    
      

#################### 3 - method - stepwise-bidirectional backward elimination:
selected_features_BE = BidirectionalStepwiseSelection(X_Train, y_Train, elimination_criteria = "adjr2")[0]   



#################### 4 - method - Genetive Algoritms:
# kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
# https://www.kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
from EvolutionaryFS import GeneticAlgorithmFS
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

data_dict={0:{'x_train':X_Train,'y_train':y_Train,'x_test':X_Test,'y_test':y_Test}}
columns_list=list(X_Train.columns)

model_object=LinearRegression(n_jobs=-1)
# model_object=LinearRegression(n_jobs=-1)
evoObj=GeneticAlgorithmFS(model=model_object,data_dict=data_dict,\
                          cost_function=mean_squared_error,\
                          average='',\
                          cost_function_improvement='decrease',\
                          columns_list=columns_list,\
                          generations=100,\
                          population=50,\
                          prob_crossover=0.9,\
                          prob_mutation=0.1,\
                          run_time=60000)
selected_features_BE__finall=evoObj.GetBestFeatures()    

#################### 4 - method - Recursive Feature Elimination
# https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
model = LinearRegression()


#no of features
nof_list=np.arange(1, X_Train.shape[1] )            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LinearRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_Train, y_Train)
    X_test_rfe = rfe.transform(X_Test)
    model.fit(X_train_rfe, y_Train)
    score = model.score(X_test_rfe, y_Test)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
        
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))


cols = list(X_Train.columns)
model = LinearRegression()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_Train,y_Train)  
#Fitting the data to model
model.fit(X_rfe,y_Train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

#%% Update X sets  and LagList

selected_features_BE__finall = \
KeepBasicIndeptVarAndDummies(X_Train, selected_features_BE,\
                                 DummyForCol = 'month',\
                                 KeepBasicIndept = True,\
                                 KeepDummies = True)


#%%

X_Train = X_Train.filter(items=selected_features_BE__finall)
X_Test = X_Test.filter(items=selected_features_BE__finall)

LagsList = { var : LagNr for var, LagNr in LagsList.items()\
                                    if var in selected_features_BE__finall}


