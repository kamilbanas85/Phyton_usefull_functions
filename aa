#%% Selecting Futures - Lags:

##################### 1 - method - backward elimination:
selected_features_BE = BackwardEliminationPvalue(\
                                    X_Train = X_Train, y_Train = y_Train)
  
    
##################### 2 - method - stepwise-bidirectional elimination:
selected_features_BE = stepwise_selection(X_Train, y_Train)    
      

#################### 3 - method - stepwise-bidirectional backward elimination:
selected_features_BE = BidirectionalStepwiseSelection(X_Train, y_Train, elimination_criteria = "adjr2")[0]   



#################### 4 - method - Genetive Algoritms:
# kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
# https://www.kaggle.com/azimulh/feature-selection-using-evolutionaryfs-library
from EvolutionaryFS import GeneticAlgorithmFS
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

data_dict={0:{'x_train':X_Train,'y_train':y_Train,'x_test':X_Test,'y_test':y_Test}}
columns_list=list(X_Train.columns)

model_object=LinearRegression(n_jobs=-1)
# model_object=LinearRegression(n_jobs=-1)
evoObj=GeneticAlgorithmFS(model=model_object,data_dict=data_dict,\
                          cost_function=mean_squared_error,\
                          average='',\
                          cost_function_improvement='decrease',\
                          columns_list=columns_list,\
                          generations=100,\
                          population=50,\
                          prob_crossover=0.9,\
                          prob_mutation=0.1,\
                          run_time=60000)
selected_features_BE__finall=evoObj.GetBestFeatures()    

#################### 4 - method - Recursive Feature Elimination
# https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
model = LinearRegression()


#no of features
nof_list=np.arange(1, X_Train.shape[1] )            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    model = LinearRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_Train, y_Train)
    X_test_rfe = rfe.transform(X_Test)
    model.fit(X_train_rfe, y_Train)
    score = model.score(X_test_rfe, y_Test)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
        
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))


cols = list(X_Train.columns)
model = LinearRegression()
#Initializing RFE model
rfe = RFE(model, 10)             
#Transforming data using RFE
X_rfe = rfe.fit_transform(X_Train,y_Train)  
#Fitting the data to model
model.fit(X_rfe,y_Train)              
temp = pd.Series(rfe.support_,index = cols)
selected_features_rfe = temp[temp==True].index
print(selected_features_rfe)

#%% Update X sets  and LagList

selected_features_BE__finall = \
KeepBasicIndeptVarAndDummies(X_Train, selected_features_BE,\
                                 DummyForCol = 'month',\
                                 KeepBasicIndept = True,\
                                 KeepDummies = True)


#%%

X_Train = X_Train.filter(items=selected_features_BE__finall)
X_Test = X_Test.filter(items=selected_features_BE__finall)

LagsList = { var : LagNr for var, LagNr in LagsList.items()\
                                    if var in selected_features_BE__finall}
                                    
                                    
                                    
 ######################################################################################
 
 def SplitIndexIncludingBatch(DF, BatchSize,\
                            TestSplitInd,\
                            ValSplitInd,\
                            WindowLength = None):
    '''
    Function return dates of Validation and Test Sets for selected Batch size.
    Accualy it return values closest to selected values.
    
    Function calculate length of the DataFrame including future cut by WindowLength.
    If data length is not divided by batch size function cut the data.
    Then function prepare length of Train set based on specified date. However, if length of 
    train set is not divided by batch size, than it cuts data from the end of set.
    
    Args:
        DF (pd.DataFrame): Oryginal DataFrame with all observations
        BatchSize (int): batch size
        TestSplitInd (str): String representing date of Start of Test Set
        ValSplitInd (str): String representing date of Start of Test Set
        WindowLength (int): length of lags widnow

    Returns:
        (Train.index, Val.index, Test.index) (tuple(indexdatetime, indexdatetime, indexdatetime)):
                3 indexs representing train, validaton and test sets
            
    '''
     
    
    
    DFfinall = DF.copy()
    TotalLength = DF.shape[0]

    if WindowLength is not None:
        DFfinall = DFfinall.iloc[(WindowLength-1):]
        TotalLength = DFfinall.shape[0]
        
    if TotalLength % BatchSize != 0:
#       print('Batch Size should be divisor of the data length')
        DFfinall = DFfinall.iloc[(TotalLength%BatchSize):]
        
    Train, RestDF  = TrainTestSets(DFfinall, ValSplitInd)
    if Train.shape[0] % BatchSize != 0:      
        Train = Train.iloc[:-(Train.shape[0] % BatchSize)]
        RestDF = DFfinall[Train.index[-1]:].iloc[1:]
    
    Val, Test = TrainTestSets(RestDF, TestSplitInd)
    if Val.shape[0] % BatchSize != 0:    
        Val = Val.iloc[:-(Val.shape[0] % BatchSize)]
        Test = RestDF[Val.index[-1]:].iloc[1:]    

    return (Train.index, Val.index, Test.index)


################################################
################################################
################################################

Train,Val, Test = CheckPossibilityOfBatch(AnalysisData, BatchSize = 133,\
                         TestSplitInd = TestSetDate,\
                         ValSplitInd = ValSetDate,\
                         WindowLength = 52)
    
    AnalysisData.head(50)
len(Train), len(Val), len(Test)








list(A1.index)[2:]
A1[A1.index[-3]:].iloc[1:]
    
def PrepareDataForLSTMregression(DataDF, DependentVar, IndependentVar,\
                                 TestSplitInd, \
                                 ValSplitInd,\
                                 BatchSize = None,\
                                 WindowLength = None,\
                                 WidndowForDependentVar = False,\
                                 DummyForCol = None,\
                                 ScalerType = None,\
                                 ScalerRange = (0,1)):

    # DependentVar <- str, IndependentVar <- list of str
    # ScalerType <- 'MinMax' or 'Standard'
    # ScalerRange = (0,1), or (-1,1)    - tuple
    
    DataToReturn = []
       
    # Select columns of DataFrame:    
    DF = DataDF.copy().loc[:, [DependentVar] + IndependentVar]
    
    # CREATE DUMMY VARIABLES FOR 'DummyForCol' COLUMNS

    DF = CreateDummyForColumns(DF, DummyForCol)
    
                
    ### SPLIT INTO DEPENDENT AND INDEPANEDNT VARIABLES
    DF_y = DF.loc[:, [DependentVar] ]
    DF_X = DF.drop(columns= [DependentVar] )
     
    
      
    ### DIVIDE INTO TRAIN, VALIDATION AND TEST SETS    
    TrainDF_y, TestDF_y = TrainTestSets(DF_y, TestSplitInd)
    TrainDF_X, TestDF_X = TrainTestSets(DF_X, TestSplitInd)
    if ValSplitInd is not None:
        TrainDF_y, ValDF_y  = TrainTestSets(TrainDF_y, ValSplitInd)
        TrainDF_X, ValDF_X  = TrainTestSets(TrainDF_X, ValSplitInd)


    # Return data if NO scaling or SCALE DATA:
    if ScalerType is None:
        # Create list of DF to return
        DataToReturn = [TrainDF_X, TrainDF_y,\
                        TestDF_X, TestDF_y]
        if ValSplitInd is not None:
            DataToReturn =\
                DataToReturn[:2] + [ValDF_X, ValDF_y] + DataToReturn[2:]
                
    elif ScalerType is not None:
        # Define Scalers:        
        if ScalerType == 'MinMax':    
            scaler_y = MinMaxScaler( feature_range = ScalerRange )
            scaler_X = MinMaxScaler( feature_range = ScalerRange )        
        elif ScalerType == 'Standard':    
            scaler_y = StandardScaler()
            scaler_X = StandardScaler()
    
        # Fit scalers on Trains Sets:
        scaler_y = scaler_y.fit(TrainDF_y)
        scaler_X = scaler_X.fit(TrainDF_X)    

        # Scale And Convert to DataFrame Train and Test sets:        
        Train_y_sld = ScaleThenConvertArrayToDF(TrainDF_y, scaler_y)
        Train_X_sld = ScaleThenConvertArrayToDF(TrainDF_X, scaler_X)

        Test_y_sld = ScaleThenConvertArrayToDF(TestDF_y, scaler_y)
        Test_X_sld = ScaleThenConvertArrayToDF(TestDF_X, scaler_X)
        
        # Create data to return:
        DataToReturn = [Train_X_sld, Train_y_sld,\
                        Test_X_sld, Test_y_sld,\
                        scaler_X, scaler_y]
        
        if ValSplitInd is not None:
            # Scale And Convert to DataFrame Val sets        
            Val_y_sld = ScaleThenConvertArrayToDF(ValDF_y, scaler_y)
            Val_X_sld = ScaleThenConvertArrayToDF(ValDF_X, scaler_X)
            
            # Extend data to return:
            DataToReturn =\
                DataToReturn[:2] + [Val_X_sld, Val_y_sld] + DataToReturn[2:]
          
                
    return tuple(DataToReturn)


#############################################################
#############################################################
#############################################################

#%% import libraries

import numpy as np
import pandas as pd
import os
import requests
import matplotlib.pyplot as plt


from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error


#%%

MainDirectory = os.path.abspath(os.path.dirname(__file__))
os.chdir(MainDirectory)

FunctionsDirectory = 'XXX'
os.chdir(FunctionsDirectory)

from Download_Data_From_sql_DB import Download_Data_From_AzureDB_Alchemy

from Cleaning_Data_Functions import *
from TimeSeries_Forcasting_Functions import *
from Prepare_Data_For_Regression import *
from Make_TS_Regression import *
from Goodness_Of_Fit import *
from ANN_Keras_functions import *
from Feature_Selection import *
from Multicollinearity_Check_Functions import *
from CV_functions import *

os.chdir(MainDirectory)


#######################################################
#%% Read the CSV file
consumption_raw = pd.read_csv('Data.csv', parse_dates=['dateCET'])

#######################################################
#%%

consumption = consumption_raw[['dateCET','Consumption', 'Temp_avg', 'windspeed_10m_max']]
consumption.set_index('dateCET', inplace=True)

consumption[['Consumption']].plot()

consumption = consumption.asfreq('D')
consumption.isna().sum()

consumption.interpolate(method='linear', inplace=True)
#%%

from workalendar.europe import Germany
callWorkDayData = Germany()


TempVarName = 'Temp_avg'

consumption = consumption \
    .assign(week = lambda x: x.index.isocalendar().week)  \
    .assign(week = lambda x: np.where( x['week']==53, 1, x['week']))\
    .assign(HDD  = lambda x: np.where( x[TempVarName] <= 15,
                                 ( 18-x[TempVarName] ).round(1).astype(float), 0 ),
            CDD  = lambda x: np.where( x[TempVarName] >= 24,
                                 ( x[TempVarName]-21 ).round(1).astype(float), 0 ) )\
    .assign(WorkDay = lambda x: x.index.to_series().transform(lambda y:\
                              callWorkDayData.is_working_day(y)).astype(int) )




##########################################################
#%% Prepare data
##########################################################

# Select Main Data
Dependent_Var = 'Consumption'

Independent_Vars = ['Temp_avg',
                    'windspeed_10m_max',
                    'CDD',
                    'WorkDay',
#                   'day',
                    'week'
#                   'month'
                   ]

DummyForColumn = ['week']
LagList = None

#%%

analysis_data = consumption.copy()
test_date = '2022-06'

# Prepare Data For ANN
X, y =  DevideOnXandY_CreateDummies(analysis_data, 
                                   DependentVar = Dependent_Var,
                                   IndependentVar = Independent_Vars,
                                   DummyForCol = DummyForColumn,
                                   drop_first = False)

X_Train_sld, y_Train_sld,\
X_Test_sld, y_Test_sld,\
scaler_X, scaler_y = \
            PrepareDataForRegression(X, y, 
                                     TestSplitInd = test_date,
                                     ValSplitInd = None,     
                                     ScalerType = 'MinMax',
                                     ScalerRange = (0,1),                             
                                     BatchSize = None,
                                     WindowLength = 1)
            
            
import tensorflow as tf

y_Test = analysis_data.loc[y_Test_sld.index][[Dependent_Var]]


##########################################################
##########################################################
#%% FIT MODEL - hyperparameter tuning
##########################################################
##########################################################

# Define parameters set


ann_param_grid = dict(epochs = [10,20,30,50,70,90,110],
                      batch_size = [10,20,40, 60,80],
                      model__loss = ['mean_squared_error'],
                      model__optimizer = ['adam','nadam'],
                      model__neurons_nr = [25, 50, 150, 200, 250],
                      model__hidden_layers_nr = [1,2,3,4,5],
                      model__input_shape = [(X_Train_sld.shape[1], )],
                      model__add_batch_norm = [False, True],
                      model__activation_fun = ['relu', 'LeakyReLU', 'elu', 'swish'],
                      model__activation_out = ['linear'],
                      model__dropout = [0.0, 0.1, 0.2, 0.3],
                      model__init = ['glorot_uniform','normal'],
                      model__regression_type = [True])

#Final_Model_01.reset_states()

# Define wrapper
wraped_ann_model = KerasRegressor(build_fn = create_feed_forward_model)

# Search hyperparamers
model_ann_random_search = RandomizedSearchCV(wraped_ann_model,\
                                             ann_param_grid,\
                                             n_iter = 20,\
                                             cv = TimeSeriesSplit(n_splits=5).split(X_Train_sld),\
                                             verbose=1,\
                                             n_jobs=1)

model_ann_random_search.fit( X_Train_sld, y_Train_sld )

# Print the best parameters
print("Best parameters found: ", model_ann_random_search.best_params_)

###########################################
#%% Test Set Prediction

model_finall = model_ann_random_search.best_estimator_

# Make prediction on test set
yhat_Test_ANN, X_Test_ANN = \
     MakeTSforecast(X_Test_sld,\
                    Model = model_finall,\
                    DependentVar = Dependent_Var,\
                    Intecept = False,\
                    LagsList = LagList,\
                    Scaler_y = scaler_y,\
                    Scaler_X = scaler_X,\
                    Test_or_Forecast = 'Test')

data_with_prediction= \
    MakeANNfinalData(Model = model_finall,\
                    Train_X_Scaled = X_Train_sld,\
                    Val_X_Scaled = None,\
                        Scaler_y= scaler_y,\
                        MainDF = analysis_data,\
                        yhat_Test_DF = None,\
                        yhat_Forecast_DF = None)

###########################################
#%% Plot Fitted Data

#CalculateR2andR2adjForSatModelsWithFormula(ConTest, MainVar, ConModelShort)

print( 'MAE: ' + str(round( mean_absolute_error( y_Test, yhat_Test_ANN), 2) ))
print( 'MAPE: ' + str(round( 100*mean_absolute_percentage_error( y_Test, yhat_Test_ANN), 2)) )
print( 'RMSE: ' + str(round( np.sqrt(mean_squared_error( y_Test, yhat_Test_ANN)), 2) ))

# plot
pd.concat([ data_with_prediction[[Dependent_Var]],\
            data_with_prediction[['Fitted-Train']],\
             yhat_Test_ANN], axis=1).plot()     

plt.ylabel('Demand', fontsize=14)
plt.xlabel('Date', fontsize=14)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.title('Winter - test set', fontsize=20)
plt.legend(fontsize=14)
plt.grid()
plt.rcParams['figure.figsize'] = [15, 10]
plt.show()

# %%
