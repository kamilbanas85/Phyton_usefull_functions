import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import requests
import json

from plotnine import *


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import BatchNormalization


from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV


# pip install plotnine --user


MainDirectory = os.path.abspath(os.path.dirname(__file__))
os.chdir(MainDirectory)

FuncDirectory = MainDirectory + '\\Usefull_Functions'
os.chdir(FuncDirectory)

# from Forecasting_Functions import *

os.chdir(MainDirectory)


Prepare_Data_For_Regression =\
'https://raw.githubusercontent.com/kamilbanas85/Phyton_usefull_functions/main/Prepare_Data_For_Regression.py'
Make_TS_Regression =\
'https://raw.githubusercontent.com/kamilbanas85/Phyton_usefull_functions/main/Make_TS_Regression.py'
Goodness_Of_Fit =\
'https://raw.githubusercontent.com/kamilbanas85/Phyton_usefull_functions/main/Goodness_Of_Fit.py'
ANN_Keras_functions =\
'https://raw.githubusercontent.com/kamilbanas85/Phyton_usefull_functions/main/ANN_Keras_functions.py'



exec(requests.get(Prepare_Data_For_Regression).text)
exec(requests.get(Make_TS_Regression).text)
exec(requests.get(Goodness_Of_Fit).text)
exec(requests.get(ANN_Keras_functions).text)



#%% Read Data


from Storage_TS import StorageDF 


#%%  Preapare data for ANN


StorageDF_ANN = StorageDF.copy()

      
# StorageDF_ANN.info()

DependentVariable = 'withdrawal'
IndependentVariables = ['StorageCapacity', 'HDD', 'week']
DummyForColumn = 'week'
TestSetDate = '2019-06'
ValSetDate = '2020'

'''
### Add LAGS

LagsRangeList = {'gasInStorage':6}
LagsDirectList = {'gasInStorage':[2,4], 'withdrawal':[1,6]}


LagsList = MakeLagedVariableNames(LagsRangeList = LagsRangeList,\
                                  LagsDirectList = None)  
    
StorageDF_ANN =   PrepareLags(StorageDF_ANN,\
                              LagsList = LagsList)

IndependentVariables.extend( LagsList.keys() )
    
'''


Train_X_sld, Train_y_sld,\
Val_X_sld, Val_y_sld,\
Test_X_sld, Test_y_sld,\
scaler_X, scaler_y = \
PrepareDataForRegression(StorageDF_ANN,\
                         DependentVar = DependentVariable, \
                         IndependentVar = IndependentVariables,\
                         DummyForCol = DummyForColumn,\
                         TestSplitInd = TestSetDate,\
                         ValSplitInd = ValSetDate,\
                         ScalerType = 'MinMax',\
                         ScalerRange = (0,1) )

     


#%% HyperParameter Tunning
    
# A larger network requires more training and at least
# the batch size and number of epochs should ideally be optimized
# with the number of neurons.

    
# kernel_initializer = 'uniform'
# kernel_constraint=maxnorm(4)
#     model.add(Dropout(0.2))


## 01

ParameterForSearch = dict(HiddenLayersNumber = [1,2,3,4,5],\
                          NeuronsNumber=[25, 50, 150, 200,250],\
                          InputShape = [(Train_X_sld.shape[1], )],\
                          AddBatchNorm = [False],\
                          LossFun = ['mean_squared_error'],\
                          Opt = ['adam'],\
                          ActivationFun = ['relu'],
                          ActivationOut = ['linear'],\
                          nb_epoch = [10,20,30],\
                          batch_size = [10,20,50,])

# Epoch Numers and Batch Size List - in parameter list
# but outside function definition. If not it can be specify in:
#    
# WrapedModel01 = KerasRegressor(build_fn = CreateFeedForwardModel,\
#                                nb_epoch = 100,\
#                                batch_size=10)

WrapedANNmodel = KerasRegressor(build_fn = CreateFeedForwardModel)
#ANNmodel_GridSearch = GridSearchCV(WrapedANNmodel, ParameterForSearch, cv =3)
ANNmodel_RandomSearch =  RandomizedSearchCV(WrapedANNmodel,\
                                            ParameterForSearch, cv =3)
ANNmodel_RandomSearchFitted = ANNmodel_RandomSearch.fit(\
                                            Train_X_sld, Train_y_sld)

ANNmodel_RandomSearchFitted.best_params_


### Final Model

# https://datascience.stackexchange.com/questions/45810/what-is-gridsearchcv-doing-after-it-finishes-evaluating-the-performance-of-param/45817

# ResultsRandomModel.best_params_

Final_Model = CreateDeepLearningFeedForwardModel(\
                   HiddenLayersNumber=\
                      ResultsRandomModel.best_params_['HiddenLayersNumber'],\
                   NeuronsNumber=\
                      ResultsRandomModel.best_params_['NeuronsNumber'],\
                   InputShape =\
                      ResultsRandomModel.best_params_['InputShape'],\
                   AddBatchNorm = False,\
                   LossFun = 'mean_squared_error',\
                   Opt = 'adam',\
                   ActivationFun = 'relu')



Final_Model__Fitted = Final_Model.fit(Train_X_sld, Train_y_sld,\
                                  validation_data= (Val_X_sld, Val_y_sld),\
                                  epochs=\
                              ResultsRandomModel.best_params_['nb_epoch'],\
                                  batch_size=\
                              ResultsRandomModel.best_params_['batch_size'])


PlotLossTrainVsVal(Final_Model__Fitted)

DataWithPrediction = \
    MakePrediction(Final_Model,\
                   Train_X_sld, Val_X_sld, Test_X_sld,\
                   scaler_y, StorageDF_ANN,\
                   SplitDataInd = DatesForTrainValAndTestSets)


DataWithPrediction[['Fitted-Train','Fitted-Validation','Predicted-Test',\
                    DependentVariable]]\
                 .plot()



