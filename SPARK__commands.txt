https://www.sqlshack.com/

###################################

from pyspark.sql import functions as F

for col in [MainVar, VarName]:
  VarDataFrom__Long_term_modified = VarDataFrom__Long_term_modified.withColumn(
    col,
    F.col(col).cast("double")
  )
  




###### upgrate packeges to newst version

%sh /databricks/python3/bin/pip install --upgrade --no-cache-dir --force-reinstall koalas





'''
from pyspark.sql import DataFrame

def list_dataframes():

    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]
'''
 


#########################
## save data to local Disc

spark.createDataFrame( AuctionCalendarDF.reset_index() ).write.mode("overwrite").saveAsTable("EUA_Auction_Calendar")


########################
## read data from local Disc

spark.sql("select * from temperature_forecast").toPandas() 

########################
## add column with contant value

from pyspark.sql.functions import lit

df = df('newColName', lit( someValue ) )

############################
# define function to retrurn name of columns which are nuull

from pyspark.sql.functions import countDistinct

def CheckNullColumns(df):
  
  nullColumns = []

  for k in df.columns:
    if df.agg(countDistinct(df[k])).take(1)[0][0] == 0:
      nullColumns.append(k)
      
  return nullColumns

############################
## chenge column type

df("colName",df["colName"].cast(StringType()))

## select first element from colmn as scalar value
              

var = df.first()['colName']